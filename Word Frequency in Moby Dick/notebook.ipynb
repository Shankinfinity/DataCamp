{"cells":[{"source":"![mobydick](mobydick.jpg)","metadata":{},"id":"b1309988-b429-4fb0-8c4c-193582dbec93","cell_type":"markdown"},{"source":"In this workspace, you'll scrape the novel Moby Dick from the website [Project Gutenberg](https://www.gutenberg.org/) (which contains a large corpus of books) using the Python `requests` package. You'll extract words from this web data using `BeautifulSoup` before analyzing the distribution of words using the Natural Language ToolKit (`nltk`) and `Counter`.\n\nThe Data Science pipeline you'll build in this workspace can be used to visualize the word frequency distributions of any novel you can find on Project Gutenberg.","metadata":{},"id":"611e416c-70e7-478a-a3c8-e54f3fdb4a7f","cell_type":"markdown"},{"source":"Project Instructions\n\nWhat are the most frequent words in Herman Melville's novel Moby Dick, and how often do they occur?\n\nNote that the HTML file you are asked to request is a cashed version of this file from Project Gutenberg.\n\nYour project will follow these steps:\n\n    The first step will be to request the Moby Dick HTML file using requests and encoding it to utf-8. Here is the URL to scrape from: https://s3.amazonaws.com/assets.datacamp.com/production/project_147/datasets/2701-h.htm\n    Next, you'll extract the HTM\n\nL and create a BeautifulSoup object using an HTML parser to get the text.\nFollowing that, you'll initialize a regex tokenizer object tokenizer using nltk.tokenize.RegexpTokenizer to keep only alphanumeric text, assigning the results to tokens.\nYou'll transform the tokens into lowercase, removing English stop words, and saving the results to words_no_stop.\nFinally, you'll initialize a Counter object and find the ten most common words, saving the result to top_ten and printing to see what they are.","metadata":{},"cell_type":"markdown","id":"d5499a48-eaee-4dd8-b861-692da28ce700"},{"source":"# Import and download packages\nimport requests\nfrom bs4 import BeautifulSoup\nimport nltk\nfrom collections import Counter\nnltk.download('stopwords')\n\n# Start coding here... ","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"executionTime":55,"lastSuccessfullyExecutedCode":"# Import and download packages\nimport requests\nfrom bs4 import BeautifulSoup\nimport nltk\nfrom collections import Counter\nnltk.download('stopwords')\n\n# Start coding here... ","executionCancelledAt":null,"lastExecutedAt":1760103327158,"lastExecutedByKernel":"67c7c7c7-def2-4e0a-8abd-4a6bc153f6c7","lastScheduledRunId":null,"outputsMetadata":{"0":{"height":59,"type":"stream"}}},"id":"15b5f52f-fd9b-4f0e-9fcc-f7733022c7c0","cell_type":"code","execution_count":67,"outputs":[{"output_type":"stream","name":"stderr","text":"[nltk_data] Downloading package stopwords to /home/repl/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"},{"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{},"execution_count":67}]},{"source":"url = \"https://s3.amazonaws.com/assets.datacamp.com/production/project_147/datasets/2701-h.htm\"\nr = requests.get(url)\nr.encoding = 'utf-8'\nhtml = r.text\nhtml_soup = BeautifulSoup(html, \"html.parser\")\nmoby_text = html_soup.get_text()","metadata":{"executionCancelledAt":null,"executionTime":424,"lastExecutedAt":1760103327582,"lastExecutedByKernel":"67c7c7c7-def2-4e0a-8abd-4a6bc153f6c7","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"url = \"https://s3.amazonaws.com/assets.datacamp.com/production/project_147/datasets/2701-h.htm\"\nr = requests.get(url)\nr.encoding = 'utf-8'\nhtml = r.text\nhtml_soup = BeautifulSoup(html, \"html.parser\")\nmoby_text = html_soup.get_text()","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"cell_type":"code","id":"26c4b2dd-5a4f-4848-9f42-1794dda49315","outputs":[],"execution_count":68},{"source":"from nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\n\ntokenizer = RegexpTokenizer(r'\\w+')\ntokens = tokenizer.tokenize(moby_text)\n\nwords = [word.lower() for word in tokens]\nstop_words = nltk.corpus.stopwords.words('english')\n\nwords_no_stop = [word for word in words if word not in stop_words]\n\ncount = Counter(words_no_stop)\ntop_ten = count.most_common(10)\n\ntop_ten","metadata":{"executionCancelledAt":null,"executionTime":540,"lastExecutedAt":1760103328125,"lastExecutedByKernel":"67c7c7c7-def2-4e0a-8abd-4a6bc153f6c7","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\n\ntokenizer = RegexpTokenizer(r'\\w+')\ntokens = tokenizer.tokenize(moby_text)\n\nwords = [word.lower() for word in tokens]\nstop_words = nltk.corpus.stopwords.words('english')\n\nwords_no_stop = [word for word in words if word not in stop_words]\n\ncount = Counter(words_no_stop)\ntop_ten = count.most_common(10)\n\ntop_ten","outputsMetadata":{"0":{"height":248,"type":"stream"}}},"cell_type":"code","id":"ae789125-30ad-42e9-a520-2836b5c367f6","outputs":[{"output_type":"execute_result","data":{"text/plain":"[('whale', 1246),\n ('one', 925),\n ('like', 647),\n ('upon', 568),\n ('man', 527),\n ('ship', 519),\n ('ahab', 517),\n ('ye', 473),\n ('sea', 455),\n ('old', 452)]"},"metadata":{},"execution_count":69}],"execution_count":69}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}